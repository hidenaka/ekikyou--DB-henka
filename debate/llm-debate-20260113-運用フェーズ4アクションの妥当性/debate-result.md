# LLM Debate: 運用フェーズ4アクションの妥当性

## 議題
運用フェーズ4アクションの妥当性

## Claude (Anthropic) の見解
Phase0-3完了後の運用フェーズとして4アクションを計画。(1)評価セット500件の二重ラベリングでKPI基盤構築(300時間)、(2)Wikipedia引用抽出でpointer_only事例を昇格(16時間)、(3)主張タグ付け100件パイロットでCore Claims検証(50時間)、(4)Unknown率を88.5%→50%に削減(750時間)。推奨順序はアクション2→1→3→4。自動化可能なWikipediaを先行し、人手作業は後続。

## Codex (OpenAI GPT-5.2) の批評
### 結論（最初に明言）
この運用フェーズ4アクション計画は、方向性は理解できるが**現状のままでは妥当ではない**。理由は、(i) **KPI定義が誤っている**（Unknown率を目標化する設計が破綻）、(ii) **評価指標が成立していない**（ECE/Brierの前提が欠落）、(iii) **「Wikipedia降下」が証拠化になっていない**（外部リンク抽出＝根拠獲得ではない）、(iv) **人手工数見積が構造的に過小**で、実行しても再現性ある成果に収束しないからである。

### 批判的分析
#### 1) 論理的整合性（前提・定義が壊れている）
- **Unknown率=悪という前提が誤りである。**「unverifiable」は世界の性質であり、運用で消せるノイズではない。Unknown率を下げる最短経路は「検証しやすい事例だけを選別する」ことであり、これはデータの代表性を破壊してモデル評価と運用判断を誤らせる。
- **Proper Scoring Rule（ECE/Brier）の計画が成立していない。**Brier Score/ECEは「確率予測」が前提である。計画には「各事例に対する予測確率（いつ・誰が・どのモデルが出したか）」が存在しない。したがって「完了後にECE/Brier計算」は現時点で虚偽の工程である。
- **ラベル設計が重複・矛盾している。**  
  - `outcome_verified: correct/incorrect/unverifiable` と `factual_accuracy: accurate/minor_error/major_error/unverifiable` は概念が衝突する。例えば「minor_errorだが結論はcorrect」の許容範囲が未定義で、ラベラー間の不一致を確実に増やす。  
  - `source_quality` のカテゴリ（primary/secondary/tertiary/unreliable）と、後段で使う `tier1/tier2`、Action2の `primary/secondary/other` が混在し、スキーマが運用上の単一言語になっていない。
- **依存関係の設計が甘い。** Action2の出力（リンク分類）は、Action1の「何を根拠と認めるか」という定義が固まっていないと価値が測れない。順序を「自動化だから先」で決めるのは運用計画として幼稚である。

#### 2) 実現可能性（隠れたコストを無視している）
- **Action1（500件二重ラベリング300h）は過小見積である。**  
  15分/件は「ソース探索・本文確認・矛盾解消・記録・根拠URLの保存・判断理由の要約」を含めると成立しない。さらに、ラベラー教育、ガイドライン整備、ツール整備（入力UI/差分比較/監査ログ）、QA再ラベリングが必須で、ここが0時間扱いになっている時点で見積の信頼性がない。
- **不一致20%は根拠がない。** この種の複合ラベル（正誤＋ソース品質＋COI＋誤差程度）は、初期は不一致が高くなる。20%前提で工程を組むのは失敗の原因である。
- **Action2（Wikipedia 16h）は「実装時間」しか見ていない。** 外部リンクの正規化（重複排除、リダイレクト追跡、追跡パラメータ除去）、ドメイン辞書の保守、レート制限、キャッシュ、失敗再試行、そして「そのリンクが当該主張の根拠か」の検証が未計上である。最も重いのは後者であり、ここが抜けている。
- **Action4（750h）は組織能力の前提が欠落している。** 4,500件×10分は「読み込み不要で機械的に判定できる」前提だが、検証は根拠読解が中心である。10分前提は成り立たない。

#### 3) 新規性の真偽（新しくない）
- 4アクションは「評価セット作成」「Wikipediaから外部リンク抽出」「主張アノテーション」「未検証の追加検証」であり、**データ品質運用の定番**である。新規性はゼロである。新規性を主張するなら「既存手法と比べて何が速く・安く・正確か」を定量で示す必要があるが、計画には存在しない。

#### 4) スケーラビリティ（人手前提で破綻）
- 500件二重＋100件主張抽出＋4,500件検証は、**拡大すればするほど品質が落ちる典型構造**である。人手の増員はIAAを下げ、調停コストを指数的に増やす。  
- Action2も「リンクが増えるほど手動検証が増える」ため、外部リンクを掘るだけではスケールしない。証拠化（claim↔citation紐付け）を自動化しない限りボトルネックは解消しない。

#### 5) 反例・エッジケース（計画が破綻する典型）
- **そもそも検証不能**：一次資料が存在しない、非公開、口頭伝聞のみ、時間依存（更新される統計）、定義が曖昧（「有名」「多い」）。Unknownを減らす目標はここで必ず破綻する。
- **Wikipedia外部リンクの罠**：外部リンクは参考・関連リンクであり、主張の根拠ではない。さらに、リンク切れ・アーカイブ・誘導ページ・商用サイト・SNSが混ざる。分類しても「証拠」にならない。
- **主張抽出の境界問題**：1事例3–5主張という固定は誤りである。事例によって主張数は0にも20にもなる。固定本数は情報を捏造する行為であり、後段モデルを汚染する。
- **COI判定の非再現性**：COIは「推測」と「証拠」が混ざりやすい。ガイドラインと証拠要件が無いCOIラベルはノイズ生成器である。

#### 6) 代替案との比較（より優れた設計が存在する）
この計画の欠陥は「量をKPI化」して「証拠化を未実装」のまま人手を投入する点にある。代替は以下である。
- **KPIをUnknown率から“検証可能集合での被覆率”に置換**：  
  `Unknown` を「検証不能理由つき」に分解し、目標は `verifiable_cases_coverage` と `evidence_completeness` にする。Unknown率50%は廃棄が正しい。
- **Action1は“500件二重”の前に30–50件の設計スプリントが必須**：  
  まず定義を固め、各フィールドのIAAを個別に測定し、COIなど低再現フィールドは削る。これをしない500件二重は時間を燃やすだけである。
- **Action2は externallinks 抽出ではなく“引用（citation）抽出＋主張紐付け”が本体**：  
  「リンクを拾った」ではなく「この主張はこの引用で支えられる」をデータ化する。降下成功率は“関連一次/二次根拠が主張に紐づいた比率”で定義し直す。
- **Action4は4,500件手作業ではなく“選別と自動補助”が本体**：  
  価値が高い（頻出・コア主張・影響大）事例を優先し、機械で根拠候補を提示→人が最終判断、という形に変える。人手で全文検索・読解を毎回やる設計は破綻する。

### Claudeの意見への反論/修正
- 「自動化できるWikipediaを先行」は**工程都合の最適化であり、品質最適化ではない**。外部リンク抽出は証拠化ではないので、先にやってもKPIが改善した“気分”しか得られない。Action2を先行するなら「引用紐付けの正解率評価」を同時に入れる必要がある。
- 「評価セット二重ラベリングでKPI基盤」は方向として正しい。しかし、**ECE/Brierを出す前提（確率予測ログの固定）が欠落**している点を見落としている。ここは計画上の致命傷である。
- 「Unknown率50%」目標を妥当として扱っているのは誤りである。Unknownは削減対象ではなく、分類して管理すべき状態である。

### より正確な見解
本質は「Unknownを減らす」ではなく、**(1) 検証の定義を固定し、(2) 主張と根拠を機械可読にし、(3) 評価指標が成立するログとデータを作る**ことである。したがって運用フェーズは次の形に再設計すべきである。
1. **定義と計測の確立（最優先）**：ラベル間の整合性（correct/incorrectとminor/majorの関係）を規約化し、IAA指標（どの係数か）と合格基準をフィールド別に設定する。ECE/Brierを使うなら予測確率ログを固定する。  
2. **Wikipediaは“引用→主張”の証拠グラフ化に限定**：外部リンク収集ではなく、主張を支える引用を構造化して初めて降下が価値を持つ。  
3. **主張タグ付けは測定設計込みで小さく始める**：抽出境界の一致率（スパン/F1）とコア主張一致率を測る。固定本数ルールは廃棄する。  
4. **スケールは人手総量ではなく情報利得で回す**：4,500件検証は目標から外し、「高価値事例の検証被覆率」「根拠完備率」「再現性（IAA）」をKPIに置く。

この修正なしに現計画を実行する行為は、時間と信頼性を同時に失う。


## 統合分析

### 共通点
1. **評価セット構築の重要性**: 両者ともProper Scoring Rule基盤としての評価セットが必要という点で一致
2. **主張単位の検証**: 事例全体ではなく主張レベルでの検証が本質という認識は共通
3. **人手作業の限界**: 大量の人手検証にはスケーラビリティの問題があることを認識

### 相違点
| 論点 | Claude | Codex |
|------|--------|-------|
| Unknown率 | 50%削減を目標化 | 削減対象ではなく管理対象 |
| Wikipedia活用 | 外部リンク抽出で価値 | 主張↔引用紐付けなしでは無意味 |
| ECE/Brier | 500件検証後に計算 | 予測確率ログがないと計算不能 |
| 工数見積 | 実装時間中心 | 教育・ツール・QAが欠落 |
| 主張抽出 | 1事例3-5件固定 | 固定本数は情報捏造 |
| アクション順序 | 自動化優先(A2→A1) | 定義確立優先 |

### Codex批評の妥当性評価

**受け入れるべき指摘**:
1. ✅ **ECE/Brierの前提欠落**: 確かに予測確率ログがなければProper Scoring Ruleは計算できない。これは計画の致命的欠陥
2. ✅ **Unknown率KPIの問題**: 「検証しやすい事例を選別」すればUnknownは減るが、代表性が破壊される
3. ✅ **設計スプリントの必要性**: 500件二重の前に30-50件で定義を固めるべき
4. ✅ **Wikipedia外部リンク≠証拠化**: リンク収集だけでは「どの主張をどの引用が支えるか」が不明

**検討が必要な指摘**:
1. ⚠️ **主張抽出固定本数**: 目安として3-5件は有用だが、強制ではなく柔軟に対応すべき
2. ⚠️ **750時間の全面否定**: 高価値事例の優先と機械補助は取り入れるべきだが、人手検証ゼロは現実的でない

### 結論・推奨

Codexの批評は大部分が妥当であり、現計画をそのまま実行すると「時間と信頼性を同時に失う」リスクが高い。以下のように修正して再計画する。

#### 修正版アクション計画

**フェーズα: 定義確立（最優先・2週間）**
1. ラベル間整合性の規約化（correct/incorrectとminor/majorの関係）
2. IAA指標とフィールド別合格基準の設定
3. 30-50件の設計スプリントで定義検証
4. ECE/Brierを使うなら予測確率ログの設計

**フェーズβ: 小規模パイロット（3週間）**
1. 主張タグ付け30件（固定本数なし、スパンF1測定）
2. Wikipedia引用→主張紐付けを30件で検証
3. 評価セット50件の二重ラベリングでIAA測定
4. 問題フィールド（COI等）の削除判断

**フェーズγ: 修正版本格実施**
1. 評価セット300件に縮小（500件は過大）
2. Wikipedia証拠グラフ化（外部リンク収集ではなく）
3. Unknown率は「削減」ではなく「検証可能集合での被覆率」に置換
4. 高価値事例優先＋機械補助で人手負荷軽減

#### KPI修正
| 旧KPI | 新KPI |
|-------|-------|
| Unknown率50%以下 | verifiable_cases_coverage（検証可能事例の被覆率） |
| 降下成功率30% | citation_claim_linkage_rate（引用-主張紐付け率） |
| IAA≥0.8（全体） | IAA≥0.8（フィールド別、低再現フィールドは削除） |

---
*生成日時: 2026-01-13T13:15:31.597Z*
*統合分析完了: 2026-01-13*
