# AI収集パイプライン設計書

## ビジョン

**目標**: 「あるある」感 = 普遍的なパターンが浮かび上がるリアル感
**規模**: 1万件以上（現状678件 → 10,000件）
**方針**: AIで収集 → AIで構造化 → AIで分析 の全自動化

---

## アーキテクチャ概要

```
┌─────────────────────────────────────────────────────────────┐
│                    データソース層                            │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐    │
│  │Wikipedia │  │ニュース  │  │ 書籍OCR  │  │インタビュー│    │
│  │ 人物記事 │  │  記事    │  │  データ  │  │  記事    │    │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘  └────┬─────┘    │
└───────┼──────────────┼──────────────┼──────────────┼────────┘
        │              │              │              │
        ▼              ▼              ▼              ▼
┌─────────────────────────────────────────────────────────────┐
│                    収集層（Collector）                       │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  scripts/collectors/                                  │  │
│  │  ├── wikipedia_collector.py  (Wikipedia API)         │  │
│  │  ├── news_collector.py       (ニュースAPI)           │  │
│  │  ├── book_collector.py       (OCR + パース)          │  │
│  │  └── interview_collector.py  (ウェブスクレイピング)   │  │
│  └──────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
        │
        ▼ 生テキスト（人物の経歴、出来事の記述）
┌─────────────────────────────────────────────────────────────┐
│                    構造化層（Structurer）                    │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  scripts/structurer/                                  │  │
│  │  ├── text_to_transitions.py  (テキスト→遷移抽出)     │  │
│  │  ├── transition_to_schema.py (遷移→スキーマ変換)     │  │
│  │  └── hexagram_classifier.py  (八卦自動分類)          │  │
│  │                                                       │  │
│  │  使用AI: Claude API / GPT-4 API                       │  │
│  │  プロンプト: prompts/extraction_prompt.md             │  │
│  └──────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
        │
        ▼ 構造化データ（Case形式のJSON）
┌─────────────────────────────────────────────────────────────┐
│                    検証層（Validator）                       │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  scripts/validators/                                  │  │
│  │  ├── schema_validator.py     (スキーマ準拠チェック)   │  │
│  │  ├── duplicate_checker.py    (重複検出)              │  │
│  │  ├── quality_scorer.py       (品質スコアリング)       │  │
│  │  └── human_review_queue.py   (人間レビュー振り分け)   │  │
│  │                                                       │  │
│  │  ルール:                                              │  │
│  │  - 品質スコア80%以上 → 自動承認                       │  │
│  │  - 品質スコア50-80% → サンプリングレビュー            │  │
│  │  - 品質スコア50%未満 → 破棄                          │  │
│  └──────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
        │
        ▼ 検証済みデータ
┌─────────────────────────────────────────────────────────────┐
│                    保存層（Storage）                         │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  data/raw/cases.jsonl        (メインDB)              │  │
│  │  data/pending/               (レビュー待ち)           │  │
│  │  data/rejected/              (破棄データ)             │  │
│  │  data/sources/               (元ソースのキャッシュ)   │  │
│  └──────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
```

---

## Phase 1: Wikipedia人物記事からの自動収集（MVP）

### 1.1 対象データソース

**Wikipedia日本語版の人物記事**
- 経営者カテゴリ: 約5,000記事
- 政治家カテゴリ: 約10,000記事
- スポーツ選手: 約20,000記事
- 芸能人・文化人: 約15,000記事

**選定理由**:
- 構造化されている（経歴セクションがある）
- APIで取得可能
- 信頼性が比較的高い
- 無料で大量取得可能

### 1.2 収集フロー

```python
# scripts/collectors/wikipedia_collector.py の概要

1. カテゴリAPIで人物記事一覧を取得
2. 各記事の「経歴」「来歴」セクションを抽出
3. 生テキストとして保存
4. メタデータ（記事名、URL、取得日時）を付与
```

### 1.3 構造化プロンプト

```markdown
# prompts/extraction_prompt.md

あなたは易経の八卦に基づいて人物の変化を分析する専門家です。

以下のテキストから、その人物の人生における「重要な変化・転換点」を抽出してください。

## 抽出ルール

1. 1つの変化につき1レコードを作成
2. 各レコードには以下を含める：
   - period: 時期（例: "1985-1990"）
   - story_summary: 何が起きたか（100-200文字）
   - before_state: 変化前の状態
   - trigger_type: きっかけのタイプ
   - action_type: 取った行動
   - after_state: 変化後の状態
   - before_hex / trigger_hex / action_hex / after_hex: 八卦
   - outcome: 結果（Success/Failure/Mixed/PartialSuccess）

## 八卦の対応表

| 八卦 | 状態・性質 |
|------|-----------|
| 乾 | 強さ、創造、リーダーシップ、絶頂 |
| 坤 | 基盤、受容、従順、地道 |
| 震 | 衝撃、混乱、突然の変化、始まり |
| 巽 | 柔軟、浸透、対話、適応 |
| 坎 | 試練、危機、困難、どん底 |
| 離 | 才能、明知、分離、輝き |
| 艮 | 待機、停滞、蓄積、忍耐 |
| 兌 | 喜び、成功、交流、達成 |

## before_state / after_state の選択肢

- 絶頂・慢心
- 停滞・閉塞
- 混乱・カオス
- 成長痛
- どん底・危機
- 安定・平和
- V字回復・大成功
- 縮小安定・生存

## trigger_type の選択肢

- 外部ショック
- 内部崩壊
- 意図的決断
- 偶発・出会い

## action_type の選択肢

- 攻める・挑戦
- 守る・維持
- 捨てる・撤退
- 耐える・潜伏
- 対話・融合
- 刷新・破壊
- 逃げる・放置
- 分散・スピンオフ

## 出力形式

JSON配列で出力してください。

---

入力テキスト:
{text}
```

### 1.4 品質スコアリング

```python
# scripts/validators/quality_scorer.py の概要

def calculate_quality_score(case: dict) -> float:
    score = 0

    # 必須フィールドの充実度（40点）
    if case.get('story_summary') and len(case['story_summary']) >= 50:
        score += 20
    if case.get('period'):
        score += 10
    if case.get('logic_memo'):
        score += 10

    # 八卦の整合性（30点）
    if is_valid_transition(case['before_hex'], case['after_hex']):
        score += 15
    if case['before_hex'] != case['after_hex']:  # 変化がある
        score += 15

    # ソースの信頼性（20点）
    credibility_scores = {'S': 20, 'A': 15, 'B': 10, 'C': 5}
    score += credibility_scores.get(case.get('credibility_rank', 'C'), 5)

    # 重複度（10点）
    if not is_duplicate(case):
        score += 10

    return score  # 0-100
```

---

## Phase 2: ニュース記事からの収集

### 2.1 対象ソース

- 日経新聞（私の履歴書、企業変革記事）
- 東洋経済オンライン（企業分析記事）
- ダイヤモンドオンライン（経営者インタビュー）
- NewsPicks（ストーリー記事）

### 2.2 収集方法

- RSS/APIで新着記事を監視
- 「復活」「転落」「変革」「危機」などのキーワードでフィルタ
- 記事本文を取得してAIで構造化

---

## Phase 3: 書籍OCRからの収集

### 3.1 対象

- 自伝・伝記（Amazon「ビジネス 自伝」カテゴリ）
- 企業史（社史、公式記録）
- ビジネス書（失敗学、成功法則）

### 3.2 方法

- 書籍のスキャン/写真 → OCR
- OCRテキスト → AIで事例抽出
- 著作権に注意（引用の範囲で）

---

## 実装優先順位

| Phase | 内容 | 期待件数 | 工数 |
|-------|------|----------|------|
| 1 | Wikipedia人物記事 | 3,000-5,000件 | 1週間 |
| 2 | ニュース記事 | 2,000-3,000件 | 2週間 |
| 3 | 書籍OCR | 2,000-3,000件 | 3週間 |
| 合計 | - | 7,000-11,000件 | 6週間 |

---

## コスト見積もり

### AI API コスト（Claude/GPT-4）

| 処理 | 1件あたりトークン | 1万件の総トークン | コスト（概算） |
|------|------------------|------------------|---------------|
| 構造化 | 2,000入力 + 500出力 | 2,500万トークン | $75-150 |
| 検証 | 500入力 + 100出力 | 600万トークン | $18-36 |
| 合計 | - | 3,100万トークン | $93-186 |

**月額目安**: 約1万円〜2万円（1万件処理の場合）

### インフラコスト

- ストレージ: ほぼ無料（JSONLは軽量）
- 実行環境: ローカルMacで十分

---

## 次のステップ

1. **今すぐ**: Phase 1のMVP実装（Wikipedia収集スクリプト）
2. **今週中**: 100件でテスト実行、品質確認
3. **来週**: 本番実行、1,000件を目標
4. **1ヶ月後**: 5,000件達成、Phase 2着手

---

## ファイル構成（予定）

```
scripts/
├── collectors/
│   ├── __init__.py
│   ├── wikipedia_collector.py
│   ├── news_collector.py
│   └── book_collector.py
├── structurer/
│   ├── __init__.py
│   ├── text_to_transitions.py
│   └── hexagram_classifier.py
├── validators/
│   ├── __init__.py
│   ├── schema_validator.py
│   ├── duplicate_checker.py
│   └── quality_scorer.py
└── pipeline/
    ├── __init__.py
    ├── run_collection.py      # メイン実行スクリプト
    └── config.yaml            # 設定ファイル

prompts/
├── extraction_prompt.md       # 構造化プロンプト
├── hexagram_prompt.md         # 八卦分類プロンプト
└── validation_prompt.md       # 検証プロンプト

data/
├── raw/
│   └── cases.jsonl            # メインDB（現行）
├── pending/                   # レビュー待ち
├── rejected/                  # 破棄データ
└── sources/                   # 元ソースのキャッシュ
```
